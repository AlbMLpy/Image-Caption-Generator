{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada973cc",
   "metadata": {
    "cellId": "gse4ysq618va2ib0qsv0h"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9107d0a5",
   "metadata": {},
   "source": [
    "#### Make sure you downloaded coco, flickr, glove. If not uncomment cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c95710",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!bash ../load_flickr8k.sh\n",
    "#!bash ../load_glove.sh\n",
    "#!bash ../load_coco.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ec712d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a44d0",
   "metadata": {
    "cellId": "2329yqhl9sigi74tq4a9l"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b907ca2",
   "metadata": {
    "cellId": "lpr7qbd4tyr53ufsobfp2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18126807",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you want to construct vocab yourself just ignore this cell\n",
    "\"\"\"\n",
    "# download vocab_set.pkl\n",
    "!gdown 1iOjqUxzwLGSQNYg28elCqM8EoJ6fkSna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad438383",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_from_scratch = False  # set `train_from_scratch = True` if you wish to train by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e78d3041",
   "metadata": {
    "cellId": "zkhs8shicha3zdhcr57l8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples in:\n",
      "train: 30000; validation: 1000; test: 1000\n",
      "Vocabulary size: 7707; Max length of a sentence: 40;\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from datasets.flickr8k import Flickr8kDataset\n",
    "\n",
    "\n",
    "DATASET_BASE_PATH = '../data/flickr8k/'\n",
    "VOCAB = 'vocab_set.pkl'\n",
    "\n",
    "vocab_set = None if train_from_scratch else tuple()\n",
    "\n",
    "if not train_from_scratch:\n",
    "    with open(VOCAB, 'rb') as f:\n",
    "        vocab_set = pickle.load(f)\n",
    "\n",
    "train_set = Flickr8kDataset(\n",
    "    dataset_base_path=DATASET_BASE_PATH, dist='train', vocab_set=vocab_set,\n",
    "    device=device, return_type='tensor', load_img_to_memory=False)\n",
    "\n",
    "if train_from_scratch:\n",
    "    vocab_set = train_set.get_vocab()\n",
    "    with open(VOCAB, 'wb') as f:\n",
    "        pickle.dump(vocab_set, f)\n",
    "    \n",
    "\n",
    "vocab, word2idx, idx2word, max_len = vocab_set\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "val_set = Flickr8kDataset(\n",
    "    dataset_base_path=DATASET_BASE_PATH, dist='val', vocab_set=vocab_set,\n",
    "    device=device, return_type='corpus', load_img_to_memory=False)\n",
    "\n",
    "test_set = Flickr8kDataset(\n",
    "    dataset_base_path=DATASET_BASE_PATH, dist='test', vocab_set=vocab_set,\n",
    "    device=device, return_type='corpus', load_img_to_memory=False)\n",
    "\n",
    "train_eval_set = Flickr8kDataset(\n",
    "    dataset_base_path=DATASET_BASE_PATH, dist='train', vocab_set=vocab_set,\n",
    "    device=device, return_type='corpus', load_img_to_memory=False)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"The number of samples in:\\ntrain: {len(train_set)};\"\n",
    "    + f\" validation: {len(val_set)}; test: {len(test_set)}\\n\"\n",
    "    + f\"Vocabulary size: {vocab_size}; Max length of a sentence: {max_len};\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "49ac7bbe",
   "metadata": {
    "cellId": "7qi2cph5h8rkkmptnmqg4m"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "\n",
    "train_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),  # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "eval_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
    "    transforms.CenterCrop(224),  # get 224x224 crop from random location\n",
    "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "train_set.transformations = train_transformations\n",
    "val_set.transformations = eval_transformations\n",
    "test_set.transformations = eval_transformations\n",
    "train_eval_set.transformations = eval_transformations\n",
    "\n",
    "\n",
    "eval_collate_fn = lambda batch: (torch.stack([x[0] for x in batch]), [x[1] for x in batch], [x[2] for x in batch])\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, sampler=None, pin_memory=False)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False,\n",
    "                        collate_fn=eval_collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False,\n",
    "                         collate_fn=eval_collate_fn)\n",
    "train_eval_loader = DataLoader(train_eval_set, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False,\n",
    "                               collate_fn=eval_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f630440d",
   "metadata": {
    "cellId": "qwf0bdgurlhtkxg9202nz"
   },
   "outputs": [],
   "source": [
    "start_token = word2idx['<start>']\n",
    "end_token = word2idx['<end>']\n",
    "pad_token = word2idx['<pad>']\n",
    "max_seq_len = max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9cfef93",
   "metadata": {
    "cellId": "25hze2slsnj2odpmcsvcnc"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from models.torch.densenet161_transformer import Captioner\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12ba154d",
   "metadata": {
    "cellId": "9nzf8rahfg48ugxdqkatqu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7707/7707 [00:00<00:00, 597028.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (7707, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from glove import embedding_matrix_creator\n",
    "EMBEDDING_DIM = 300\n",
    "EMBEDDING = f\"GLV{EMBEDDING_DIM}\"\n",
    "\n",
    "embedding_matrix = embedding_matrix_creator(embedding_dim=EMBEDDING_DIM, word2idx=word2idx, GLOVE_DIR='../data/glove.6B/')\n",
    "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "313edcfd",
   "metadata": {
    "cellId": "i1d8nhpmnjcpznwiwbd7ze"
   },
   "outputs": [],
   "source": [
    "final_model = Captioner(10, 6, EMBEDDING_DIM, vocab_size, embedding_matrix, False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01140255",
   "metadata": {
    "cellId": "g4j09g5vep9l5zmrms6pyp"
   },
   "outputs": [],
   "source": [
    "from train_torch import evaluate_model\n",
    "from train_transformer import train_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "83b4cdaa",
   "metadata": {
    "cellId": "3bahbup7llq48egl2x1xwk"
   },
   "outputs": [],
   "source": [
    "from utils_torch import check_create_dir\n",
    "from metrics import *\n",
    "from utils_torch import *\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e1abf38",
   "metadata": {
    "cellId": "s7qg52dw0qxuzv3jr1c0d"
   },
   "outputs": [],
   "source": [
    "MODEL = final_model.name\n",
    "\n",
    "if train_from_scratch:\n",
    "    check_create_dir(f'saved_models/{MODEL}')\n",
    "    MODEL_NAME = f'saved_models/{MODEL}/{MODEL}_b{BATCH_SIZE}_emd{EMBEDDING}'\n",
    "    NUM_EPOCHS = 20\n",
    "    print(MODEL_NAME)\n",
    "    \n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=train_set.pad_value)\n",
    "    acc_fn = accuracy_fn(ignore_value=train_set.pad_value)\n",
    "    optimizer = torch.optim.Adam(final_model.parameters(), lr=0.0005)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.8, patience=2, verbose = True)\n",
    "    min_val_loss = float('Inf')\n",
    "\n",
    "    sentence_bleu_score_fn = bleu_score_fn(4, 'sentence')\n",
    "    corpus_bleu_score_fn = bleu_score_fn(4, 'corpus')\n",
    "    tensor_to_word_fn = words_from_tensors_fn(idx2word=idx2word)\n",
    "    train_loss_min = float('Inf')\n",
    "    val_bleu4_max = 0.0\n",
    "        \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        train_loss = train_model(\n",
    "            desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}',\n",
    "            model=final_model,                                                                      \n",
    "            optimizer=optimizer,\n",
    "            loss_fn=loss_fn,                                        \n",
    "            train_loader=train_loader,\n",
    "            acc_fn=acc_fn,\n",
    "            pad_token=pad_token\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            train_bleu = evaluate_model(\n",
    "                desc=f'\\tTrain Bleu Score: ',\n",
    "                model=final_model,\n",
    "                bleu_score_fn=corpus_bleu_score_fn,\n",
    "                tensor_to_word_fn=tensor_to_word_fn,\n",
    "                data_loader=train_eval_loader,\n",
    "            )\n",
    "\n",
    "            val_bleu = evaluate_model(\n",
    "                desc=f'\\tValidation Bleu Score: ',\n",
    "                model=final_model,\n",
    "                bleu_score_fn=corpus_bleu_score_fn,\n",
    "                tensor_to_word_fn=tensor_to_word_fn,\n",
    "                data_loader=val_loader,\n",
    "            )\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{NUM_EPOCHS}',\n",
    "                ''.join([f'train_bleu{i}: {train_bleu[i]:.4f} ' for i in (1, 4)]),\n",
    "                ''.join([f'val_bleu{i}: {val_bleu[i]:.4f} ' for i in (1, 4)]),\n",
    "                )\n",
    "            \n",
    "            state = {\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': final_model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'train_loss_latest': train_loss,\n",
    "                'val_bleu4_latest': val_bleu[4],\n",
    "                'train_loss_min': min(train_loss, train_loss_min),\n",
    "                'val_bleu4_max': max(val_bleu[4], val_bleu4_max),\n",
    "                'train_bleus': train_bleu,\n",
    "                'val_bleus': val_bleu,\n",
    "            }\n",
    "\n",
    "            torch.save(state, f'{MODEL_NAME}_latest.pt')\n",
    "\n",
    "            if train_loss < train_loss_min:\n",
    "                train_loss_min = train_loss\n",
    "                torch.save(state, f'{MODEL_NAME}''_best_train_loss.pt')\n",
    "                \n",
    "            if val_bleu[4] > val_bleu4_max:\n",
    "                val_bleu4_max = val_bleu[4]\n",
    "                torch.save(state, f'{MODEL_NAME}''_best_val_bleu.pt')\n",
    "        scheduler.step(train_loss)\n",
    "        \n",
    "    torch.save(state, f'{MODEL_NAME}_ep{NUM_EPOCHS:02d}_weights.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b83bc",
   "metadata": {},
   "source": [
    "Examples of evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec70ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you want to evaluate your trained model just ignore this cell\n",
    "\"\"\"\n",
    "\n",
    "!gdown 1to3u0KEOGZEhMxZmooscUepITgyEbvxH\n",
    "!mkdir -p saved_models/densenet161_transformer\n",
    "!mv densenet161_transformer_b100_emdGLV300_best_val_bleu.pt saved_models/densenet161_transformer/densenet161_transformer_b100_emdGLV300_best_val_bleu.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6e3815",
   "metadata": {
    "cellId": "qfrqwr72dpfsa57fprslq"
   },
   "outputs": [],
   "source": [
    "PATH = f'saved_models/densenet161_transformer/densenet161_transformer_b100_emdGLV300_best_val_bleu.pt'\n",
    "final_model.load_state_dict(torch.load(PATH, map_location=torch.device(device))['state_dict'])\n",
    "final_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e57a95be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.0,\n",
       "  0.6563020474072612,\n",
       "  0.4439026354172607,\n",
       "  0.2986431441009807,\n",
       "  0.20040442408046005],\n",
       " [0.0,\n",
       "  0.6959171362081213,\n",
       "  0.4996034593920619,\n",
       "  0.3555811511011743,\n",
       "  0.25028892209102066])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.load(PATH, map_location=torch.device(device))['val_bleus'], torch.load(PATH, map_location=torch.device(device))['train_bleus']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cbce64",
   "metadata": {},
   "source": [
    "### Flickr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "81f4d08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_plot import *\n",
    "dset = test_set\n",
    "spatial = (\n",
    "    ((10, 425), (10, 465), (10, 505)),\n",
    "    ((10, 530), (10, 570), (10, 610))\n",
    ")\n",
    "\n",
    "\n",
    "idx_range = get_best_and_worst_quality_captions(dset, final_model, idx2word, bleu_score_fn(4, 'corpus'))\n",
    "\n",
    "idx2spatial = {i:v for i, v in zip(idx_range, spatial)}\n",
    "display_images_with_captions(idx2spatial, dset, final_model, idx2word, bleu_score_fn(4, 'corpus'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb692ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "notebookId": "c2a3a5aa-b4c0-4f08-bbfc-6715c7555e93",
  "notebookPath": "image_captioning/Untitled.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
