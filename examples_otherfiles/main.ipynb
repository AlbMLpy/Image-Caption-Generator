{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "from datasets.flickr8k import Flickr8kDataset\n",
    "from glove import embedding_matrix_creator\n",
    "from metrics import *\n",
    "from utils_torch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATASET_BASE_PATH = 'data/flickr8k/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 1000, 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set = Flickr8kDataset(dataset_base_path=DATASET_BASE_PATH, dist='train', device=device,\n",
    "                            return_type='tensor',\n",
    "                            load_img_to_memory=False)\n",
    "\n",
    "vocab, word2idx, idx2word, max_len = vocab_set = train_set.get_vocab()\n",
    "\n",
    "val_set = Flickr8kDataset(dataset_base_path=DATASET_BASE_PATH, dist='val', vocab_set=vocab_set, device=device,\n",
    "                          return_type='corpus',\n",
    "                          load_img_to_memory=False)\n",
    "test_set = Flickr8kDataset(dataset_base_path=DATASET_BASE_PATH, dist='test', vocab_set=vocab_set, device=device,\n",
    "                           return_type='corpus',\n",
    "                           load_img_to_memory=False)\n",
    "train_eval_set = Flickr8kDataset(dataset_base_path=DATASET_BASE_PATH, dist='train', vocab_set=vocab_set, device=device,\n",
    "                                 return_type='corpus',\n",
    "                                 load_img_to_memory=False)\n",
    "                                 \n",
    "with open('vocab_set.pkl', 'wb') as f:\n",
    "    pickle.dump(train_set.get_vocab(), f)\n",
    "len(train_set), len(val_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7708, 40)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "vocab_size, max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL = \"vgg16_monolstm\"\n",
    "EMBEDDING_DIM = 300\n",
    "EMBEDDING = f\"{EMBEDDING_DIM}\"\n",
    "ATTENTION_DIM = 256\n",
    "DECODER_SIZE = 256\n",
    "BATCH_SIZE = 10#128\n",
    "HIDDEN_SIZE = 256\n",
    "LR = 5e-4\n",
    "MODEL_NAME = f'saved_models/{MODEL}_b{BATCH_SIZE}_emd{EMBEDDING}'\n",
    "NUM_EPOCHS = 2\n",
    "SAVE_FREQ = 10\n",
    "LOG_INTERVAL = 25 * (256 // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7708/7708 [00:00<00:00, 528798.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7708, 300)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = embedding_matrix_creator(embedding_dim=EMBEDDING_DIM, word2idx=word2idx)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(train_loader, model, loss_fn, optimizer, vocab_size, acc_fn, desc=''):\n",
    "    running_acc = 0.0\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    t = tqdm(iter(train_loader), desc=f'{desc}')\n",
    "    for batch_idx, batch in enumerate(t):\n",
    "        images, captions, lengths = batch\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        scores, caps_sorted, decode_lengths, alphas, sort_ind = model(images, captions, lengths)\n",
    "\n",
    "        # Since decoding starts with <start>, the targets are all words after <start>, up to <end>\n",
    "        targets = caps_sorted[:, 1:]\n",
    "        print(targets.shape)\n",
    "\n",
    "        # Remove timesteps that we didn't decode at, or are pads\n",
    "        # pack_padded_sequence is an easy trick to do this\n",
    "        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
    "        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "        print(type(scores))\n",
    "\n",
    "        loss = loss_fn(scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_acc += (torch.argmax(scores, dim=1) == targets).sum().float().item() / targets.size(0)\n",
    "        running_loss += loss.item()\n",
    "        t.set_postfix({'loss': running_loss / (batch_idx + 1),\n",
    "                       'acc': running_acc / (batch_idx + 1),\n",
    "                       }, refresh=True)\n",
    "        if (batch_idx + 1) % LOG_INTERVAL == 0:\n",
    "            print(f'{desc} {batch_idx + 1}/{len(train_loader)} '\n",
    "                  f'train_loss: {running_loss / (batch_idx + 1):.4f} '\n",
    "                  f'train_acc: {running_acc / (batch_idx + 1):.4f}')\n",
    "\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "\n",
    "def evaluate_model(data_loader, model, loss_fn, vocab_size, bleu_score_fn, tensor_to_word_fn, desc=''):\n",
    "    running_bleu = [0.0] * 5\n",
    "    model.eval()\n",
    "    t = tqdm(iter(data_loader), desc=f'{desc}')\n",
    "    for batch_idx, batch in enumerate(t):\n",
    "        images, captions, lengths = batch\n",
    "        outputs = tensor_to_word_fn(model.sample(images, startseq_idx=word2idx['<start>']).cpu().numpy())\n",
    "\n",
    "        for i in (1, 2, 3, 4):\n",
    "            running_bleu[i] += bleu_score_fn(reference_corpus=captions, candidate_corpus=outputs, n=i)\n",
    "        t.set_postfix({\n",
    "            'bleu1': running_bleu[1] / (batch_idx + 1),\n",
    "            'bleu4': running_bleu[4] / (batch_idx + 1),\n",
    "        }, refresh=True)\n",
    "    for i in (1, 2, 3, 4):\n",
    "        running_bleu[i] /= len(data_loader)\n",
    "    return running_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/torch/nn/modules/rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "from models.torch.vgg16_monolstm import Captioner\n",
    "\n",
    "final_model = Captioner(\n",
    "    embed_size=EMBEDDING_DIM, hidden_size=HIDDEN_SIZE,\n",
    "    num_layers=1, vocab_size=vocab_size,\n",
    "    embedding_matrix=embedding_matrix, train_embd=True\n",
    ").to(device)\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=train_set.pad_value).to(device)\n",
    "acc_fn = accuracy_fn(ignore_value=train_set.pad_value)\n",
    "sentence_bleu_score_fn = bleu_score_fn(4, 'sentence')\n",
    "corpus_bleu_score_fn = bleu_score_fn(4, 'corpus')\n",
    "tensor_to_word_fn = words_from_tensors_fn(idx2word=idx2word)\n",
    "\n",
    "params = final_model.parameters()\n",
    "\n",
    "optimizer = torch.optim.Adam(params=params, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(256),  # get 256x256 crop from random location\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "eval_transformations = transforms.Compose([\n",
    "    transforms.Resize(256),  # smaller edge of image resized to 256\n",
    "    transforms.CenterCrop(256),  # get 256x256 crop from random location\n",
    "    transforms.ToTensor(),  # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),  # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "train_set.transformations = train_transformations\n",
    "val_set.transformations = eval_transformations\n",
    "test_set.transformations = eval_transformations\n",
    "train_eval_set.transformations = eval_transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_collate_fn = lambda batch: (torch.stack([x[0] for x in batch]), [x[1] for x in batch], [x[2] for x in batch])\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, sampler=None, pin_memory=False)\n",
    "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False,\n",
    "                        collate_fn=eval_collate_fn)\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False,\n",
    "                         collate_fn=eval_collate_fn)\n",
    "train_eval_loader = DataLoader(train_eval_set, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False,\n",
    "                               collate_fn=eval_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:   0%|          | 0/3000 [00:00<?, ?it/s]/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
      "Epoch 1/2:   0%|          | 0/3000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8490/2042355767.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mval_bleu4_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     train_loss = train_model(desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}', model=final_model,\n\u001b[0m\u001b[1;32m      5\u001b[0m                              \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0macc_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                              train_loader=train_loader, vocab_size=vocab_size)\n",
      "\u001b[0;32m/tmp/ipykernel_8490/1885080304.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, model, loss_fn, optimizer, vocab_size, acc_fn, desc)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaps_sorted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# Since decoding starts with <start>, the targets are all words after <start>, up to <end>\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "source": [
    "train_loss_min = 100\n",
    "val_bleu4_max = 0.0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_model(desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}', model=final_model,\n",
    "                             optimizer=optimizer, loss_fn=loss_fn, acc_fn=acc_fn,\n",
    "                             train_loader=train_loader, vocab_size=vocab_size)\n",
    "    with torch.no_grad():\n",
    "        train_bleu = evaluate_model(desc=f'\\tTrain Bleu Score: ', model=final_model,\n",
    "                                    loss_fn=loss_fn, bleu_score_fn=corpus_bleu_score_fn,\n",
    "                                    tensor_to_word_fn=tensor_to_word_fn,\n",
    "                                    data_loader=train_eval_loader, vocab_size=vocab_size)\n",
    "        val_bleu = evaluate_model(desc=f'Validation Bleu Score: ', model=final_model,\n",
    "                                  loss_fn=loss_fn, bleu_score_fn=corpus_bleu_score_fn,\n",
    "                                  tensor_to_word_fn=tensor_to_word_fn,\n",
    "                                  data_loader=val_loader, vocab_size=vocab_size)\n",
    "        print(f'Epoch {epoch + 1}/{NUM_EPOCHS}',\n",
    "              ''.join([f'train_bleu{i}: {train_bleu[i]:.4f} ' for i in (1, 4)]),\n",
    "              ''.join([f'val_bleu{i}: {val_bleu[i]:.4f} ' for i in (1, 4)]),\n",
    "              )\n",
    "        #wandb.log({f'val_bleu{i}': val_bleu[i] for i in (1, 2, 3, 4)})\n",
    "        #wandb.log({'val_bleu': val_bleu[4]})\n",
    "        state = {\n",
    "            'epoch': epoch + 1,\n",
    "            'state_dict': final_model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'train_loss_latest': train_loss,\n",
    "            'val_bleu4_latest': val_bleu[4],\n",
    "            'train_loss_min': min(train_loss, train_loss_min),\n",
    "            'val_bleu4_max': max(val_bleu[4], val_bleu4_max),\n",
    "            'train_bleus': train_bleu,\n",
    "            'val_bleus': val_bleu,\n",
    "        }\n",
    "        torch.save(state, f'{MODEL_NAME}_latest.pt')\n",
    "        #wandb.save(f'{MODEL_NAME}_latest.pt')\n",
    "        if train_loss < train_loss_min:\n",
    "            train_loss_min = train_loss\n",
    "            torch.save(state, f'{MODEL_NAME}''_best_train.pt')\n",
    "            #wandb.save(f'{MODEL_NAME}''_best_train.pt')\n",
    "        if val_bleu[4] > val_bleu4_max:\n",
    "            val_bleu4_max = val_bleu[4]\n",
    "            torch.save(state, f'{MODEL_NAME}''_best_val.pt')\n",
    "            #wandb.save(f'{MODEL_NAME}''_best_val.pt')\n",
    "\n",
    "torch.save(state, f'{MODEL_NAME}_ep{NUM_EPOCHS:02d}_weights.pt')\n",
    "#wandb.save(f'{MODEL_NAME}_ep{NUM_EPOCHS:02d}_weights.pt')\n",
    "final_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t_i = 1003\n",
    "dset = train_set\n",
    "im, cp, _ = dset[t_i]\n",
    "print(''.join([idx2word[idx.item()] + ' ' for idx in model.sample(im.unsqueeze(0), word2idx['<start>'])[0]]))\n",
    "print(dset.get_image_captions(t_i)[1])\n",
    "\n",
    "plt.imshow(dset[t_i][0].detach().cpu().permute(1, 2, 0), interpolation=\"bicubic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t_i = 500\n",
    "dset = val_set\n",
    "im, cp, _ = dset[t_i]\n",
    "print(''.join([idx2word[idx.item()] + ' ' for idx in model.sample(im.unsqueeze(0), word2idx['<start>'])[0]]))\n",
    "print(cp)\n",
    "\n",
    "plt.imshow(dset[t_i][0].detach().cpu().permute(1, 2, 0), interpolation=\"bicubic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t_i = 500\n",
    "dset = test_set\n",
    "im, cp, _ = dset[t_i]\n",
    "print(''.join([idx2word[idx.item()] + ' ' for idx in model.sample(im.unsqueeze(0), word2idx['<start>'])[0]]))\n",
    "print(cp)\n",
    "\n",
    "plt.imshow(dset[t_i][0].detach().cpu().permute(1, 2, 0), interpolation=\"bicubic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    train_bleu = evaluate_model(desc=f'Train: ', model=final_model,\n",
    "                                loss_fn=loss_fn, bleu_score_fn=corpus_bleu_score_fn,\n",
    "                                tensor_to_word_fn=tensor_to_word_fn,\n",
    "                                data_loader=train_eval_loader, vocab_size=vocab_size)\n",
    "    val_bleu = evaluate_model(desc=f'Val: ', model=final_model,\n",
    "                              loss_fn=loss_fn, bleu_score_fn=corpus_bleu_score_fn,\n",
    "                              tensor_to_word_fn=tensor_to_word_fn,\n",
    "                              data_loader=val_loader, vocab_size=vocab_size)\n",
    "    test_bleu = evaluate_model(desc=f'Test: ', model=final_model,\n",
    "                               loss_fn=loss_fn, bleu_score_fn=corpus_bleu_score_fn,\n",
    "                               tensor_to_word_fn=tensor_to_word_fn,\n",
    "                               data_loader=test_loader, vocab_size=vocab_size)\n",
    "    for setname, result in zip(('train', 'val', 'test'), (train_bleu, val_bleu, test_bleu)):\n",
    "        print(setname, end=' ')\n",
    "        for ngram in (1, 2, 3, 4):\n",
    "            print(f'Bleu-{ngram}: {result[ngram]}', end=' ')\n",
    "            wandb.run.summary[f\"{setname}_bleu{ngram}\"] = result[ngram]\n",
    "        print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
