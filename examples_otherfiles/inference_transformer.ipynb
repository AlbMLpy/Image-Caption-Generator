{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fc8599e",
      "metadata": {
        "cellId": "gse4ysq618va2ib0qsv0h",
        "id": "7fc8599e"
      },
      "outputs": [],
      "source": [
        "#!g1.1\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe8aa837",
      "metadata": {},
      "source": [
        "#### Make sure you downloaded coco, flickr, glove. With `bash` command and these scripts `load_coco.sh`, `load_flickr.sh`, `load_glove.sh` you can download them"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4SVbZV160I-N",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SVbZV160I-N",
        "outputId": "0eaac7c6-9167-4d91-e5b1-a6dbc6e4484c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'image_captioning' already exists and is not an empty directory.\n",
            "mv: cannot stat '/content/image_captioning/*': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "! git clone https://github.com/tojiboyevf/image_captioning.git\n",
        "! mv  -v /content/image_captioning/* /content/  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e370de8",
      "metadata": {
        "cellId": "2329yqhl9sigi74tq4a9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e370de8",
        "outputId": "a033955b-9ac1-43ea-e280-0f4f45bea32c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#!g1.1\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8476f707",
      "metadata": {
        "cellId": "lpr7qbd4tyr53ufsobfp2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8476f707",
        "outputId": "0426d9ff-805d-456d-eede-39d68a23c14a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "#!g1.1\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de4e6d00",
      "metadata": {
        "cellId": "zkhs8shicha3zdhcr57l8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de4e6d00",
        "outputId": "8d48afdc-f912-4154-c1aa-28485b6ca4e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of samples in:\n",
            "train: 30000; validation: 1000; test: 1000\n",
            "Vocabulary size: 7708; Max length of a sentence: 40;\n"
          ]
        }
      ],
      "source": [
        "#!g1.1\n",
        "import pickle\n",
        "\n",
        "from datasets.flickr8k import Flickr8kDataset\n",
        "\n",
        "\n",
        "DATASET_BASE_PATH = 'data/flickr8k/'\n",
        "VOCAB = 'vocab_set.pkl'\n",
        "\n",
        "train_set = Flickr8kDataset(\n",
        "    dataset_base_path=DATASET_BASE_PATH, dist='train',\n",
        "    device=device, return_type='tensor', load_img_to_memory=False)\n",
        "\n",
        "\n",
        "vocab_set = train_set.get_vocab()\n",
        "with open(VOCAB, 'wb') as f:\n",
        "    pickle.dump(vocab_set, f)\n",
        "    \n",
        "\n",
        "vocab, word2idx, idx2word, max_len = vocab_set\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "val_set = Flickr8kDataset(\n",
        "    dataset_base_path=DATASET_BASE_PATH, dist='val', vocab_set=vocab_set,\n",
        "    device=device, return_type='corpus', load_img_to_memory=False)\n",
        "\n",
        "test_set = Flickr8kDataset(\n",
        "    dataset_base_path=DATASET_BASE_PATH, dist='test', vocab_set=vocab_set,\n",
        "    device=device, return_type='corpus', load_img_to_memory=False)\n",
        "\n",
        "train_eval_set = Flickr8kDataset(\n",
        "    dataset_base_path=DATASET_BASE_PATH, dist='train', vocab_set=vocab_set,\n",
        "    device=device, return_type='corpus', load_img_to_memory=False)\n",
        "\n",
        "\n",
        "print(\n",
        "    f\"The number of samples in:\\ntrain: {len(train_set)};\"\n",
        "    + f\" validation: {len(val_set)}; test: {len(test_set)}\\n\"\n",
        "    + f\"Vocabulary size: {vocab_size}; Max length of a sentence: {max_len};\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d2f2372",
      "metadata": {
        "cellId": "7qi2cph5h8rkkmptnmqg4m",
        "id": "5d2f2372"
      },
      "outputs": [],
      "source": [
        "#!g1.1\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets.coco import CoCoDataloader\n",
        "\n",
        "BATCH_SIZE = 50\n",
        "\n",
        "eval_transformations = transforms.Compose([\n",
        "    transforms.Resize(256), \n",
        "    transforms.CenterCrop(224), \n",
        "    transforms.ToTensor(), \n",
        "    transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                         (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "val_set.transformations = eval_transformations\n",
        "test_set.transformations = eval_transformations\n",
        "train_eval_set.transformations = eval_transformations\n",
        "\n",
        "eval_collate_fn = lambda batch: (torch.stack([x[0] for x in batch]), [x[1] for x in batch], [x[2] for x in batch])\n",
        "val_loader = DataLoader(val_set, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False,\n",
        "                        collate_fn=eval_collate_fn, drop_last=True)\n",
        "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False,\n",
        "                         collate_fn=eval_collate_fn, drop_last=True)\n",
        "train_eval_loader = DataLoader(train_eval_set, batch_size=BATCH_SIZE, shuffle=False, sampler=None, pin_memory=False,\n",
        "                               collate_fn=eval_collate_fn, drop_last=True)\n",
        "\n",
        "vocab_from_file = True     # if True, load existing vocab file\n",
        "split_size = 0.1\n",
        "\n",
        "coco_val_loader = CoCoDataloader(transform=eval_transformations,\n",
        "                        batch_size=BATCH_SIZE,\n",
        "                        vocab_from_file=vocab_from_file,\n",
        "                        vocab_file='./vocab_set.pkl',\n",
        "                        size=split_size,\n",
        "                        img_folder='data/coco/val2014',\n",
        "                        annotations_file='data/coco/annotations/captions_val2014.json',\n",
        "                        shuffle=True,\n",
        "                        random_seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94fcafa5",
      "metadata": {
        "cellId": "qwf0bdgurlhtkxg9202nz",
        "id": "94fcafa5"
      },
      "outputs": [],
      "source": [
        "#!g1.1\n",
        "start_token = word2idx['<start>']\n",
        "end_token = word2idx['<end>']\n",
        "pad_token = word2idx['<pad>']\n",
        "max_seq_len = max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77a8f603",
      "metadata": {
        "cellId": "25hze2slsnj2odpmcsvcnc",
        "id": "77a8f603"
      },
      "outputs": [],
      "source": [
        "#!g1.1\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "def embedding_layer(trainable=True, embedding_matrix=None, **kwargs):\n",
        "    emb_layer = nn.Embedding(**kwargs)\n",
        "    if embedding_matrix is not None:\n",
        "        emb_layer.weight = nn.Parameter(torch.from_numpy(embedding_matrix).float())\n",
        "    trainable = (embedding_matrix is None) or trainable\n",
        "    if not trainable:\n",
        "        emb_layer.weight.requires_grad = False\n",
        "    return emb_layer\n",
        "\n",
        "\n",
        "class Encoder(nn.Module): # asosan shu bn train qildik\n",
        "    def __init__(self, embed_size):\n",
        "        super().__init__()\n",
        "        self.vit = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
        "        self.embed = nn.Sequential(\n",
        "            nn.Linear(in_features=self.vit.head.out_features, out_features=embed_size),\n",
        "            nn.GELU(),\n",
        "            nn.BatchNorm1d(embed_size, momentum=0.01),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, images):\n",
        "        with torch.no_grad():\n",
        "            features = self.vit(images)\n",
        "        features = self.embed(features)\n",
        "        return features\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=40):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.pe.size(0) < x.size(0):\n",
        "            self.pe = self.pe.repeat(x.size(0), 1, 1).to(device)\n",
        "        self.pe = self.pe[:x.size(0), : , : ]\n",
        "        \n",
        "        x = x + self.pe\n",
        "        return self.dropout(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_heads,\n",
        "        decoder_layers,\n",
        "        embed_size, \n",
        "        vocab_size, \n",
        "        embedding_matrix=None, \n",
        "        train_embd=True, \n",
        "        max_len=40, \n",
        "        dropout=0.1\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.embedding = embedding_layer(num_embeddings=vocab_size, embedding_dim=embed_size,\n",
        "                                     embedding_matrix=embedding_matrix, trainable=train_embd)\n",
        "        self.pos_encoder = PositionalEncoding(embed_size, dropout)\n",
        "\n",
        "        self.decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=num_heads)\n",
        "        self.transformer = nn.TransformerDecoder(self.decoder_layer, num_layers=decoder_layers)\n",
        "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
        "        self.embedding_size = embed_size\n",
        "        self.init_weights()\n",
        "    \n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc_out.bias.data.zero_()\n",
        "        self.fc_out.weight.data.uniform_(-initrange, initrange)\n",
        "    \n",
        "    def generate_Mask(self, size, decoder_inp):\n",
        "        decoder_input_mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
        "        decoder_input_mask = decoder_input_mask.float().masked_fill(decoder_input_mask == 0, float('-inf')).masked_fill(decoder_input_mask == 1, float(0.0))\n",
        "\n",
        "        decoder_input_pad_mask = decoder_inp.float().masked_fill(decoder_inp == 0, float(0.0)).masked_fill(decoder_inp > 0, float(1.0))\n",
        "        decoder_input_pad_mask_bool = decoder_inp == 0\n",
        "\n",
        "        return decoder_input_mask, decoder_input_pad_mask, decoder_input_pad_mask_bool\n",
        "    \n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        features = features.permute(1,0,2)\n",
        "        decoder_inp_embed = self.embedding(captions)* math.sqrt(self.embedding_size)\n",
        "        \n",
        "        decoder_inp_embed = self.pos_encoder(decoder_inp_embed)\n",
        "        decoder_inp_embed = decoder_inp_embed.permute(1,0,2)\n",
        "        \n",
        "\n",
        "        decoder_input_mask, decoder_input_pad_mask, decoder_input_pad_mask_bool = self.generate_Mask(captions.size(1), captions)\n",
        "        decoder_input_mask = decoder_input_mask.to(device)\n",
        "        decoder_input_pad_mask = decoder_input_pad_mask.to(device)\n",
        "        decoder_input_pad_mask_bool = decoder_input_pad_mask_bool.to(device)\n",
        "        \n",
        "\n",
        "        decoder_output = self.transformer(tgt = decoder_inp_embed, memory = features, tgt_mask = decoder_input_mask, tgt_key_padding_mask = decoder_input_pad_mask_bool)\n",
        "        \n",
        "        final_output = self.fc_out(decoder_output)\n",
        "        \n",
        "        return final_output,  decoder_input_pad_mask\n",
        "    \n",
        "    def sample(self, features, max_len=40, topk=3, start_token=59, end_token=57, pad_token=58):\n",
        "        input_seq = torch.ones(features.size(0), max_len).type(torch.LongTensor) * pad_token\n",
        "        input_seq[:, 0] = start_token\n",
        "        input_seq = input_seq.to(device)\n",
        "        for i in range(max_len-1):\n",
        "            output, _ = self.forward(features, input_seq)\n",
        "            output = output[i, :, :]\n",
        "            predicted = output.argmax(1)\n",
        "            input_seq[:, i+1] = predicted\n",
        "        return input_seq\n",
        "\n",
        "\n",
        "class Captioner(nn.Module):\n",
        "    def __init__(self, num_heads, decoder_layers, embed_size, vocab_size, embedding_matrix=None, train_embd=True):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(embed_size)\n",
        "        self.decoder = Decoder(num_heads, decoder_layers, embed_size, vocab_size,\n",
        "                               embedding_matrix=embedding_matrix, train_embd=train_embd)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoder(images)\n",
        "        features = features.unsqueeze(1)\n",
        "        output, padding_mask = self.decoder(features, captions)\n",
        "        return output, padding_mask\n",
        "    \n",
        "    def sample(self, images, max_len=max_seq_len, topk=3, start_token=start_token, end_token=end_token, pad_token=pad_token):\n",
        "        features = self.encoder(images)\n",
        "        features = features.unsqueeze(1)\n",
        "        captions = self.decoder.sample(features=features, max_len=max_len, topk=topk,\n",
        "                                       start_token=start_token, end_token=end_token,\n",
        "                                       pad_token=pad_token)\n",
        "        return captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7de42a2f",
      "metadata": {
        "cellId": "9nzf8rahfg48ugxdqkatqu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "6d9ce88b3cbe46d0ad6c3447bebb3700",
            "9a2d688389f4491db3a365ec0d37e806",
            "306fc3e5edad4009b772c9e35e84856d",
            "648aaeed9e2b4e38bd0bfe479345d106",
            "236036b1f093498db63165faecd4df8d",
            "6df8886445454fc9ba211545a7002949",
            "fcdfbefca4a0409692c75154bb4c4146",
            "66b5ec7cead44be4ba06a11f9808ce79",
            "93dc6ed97b004f75a4d78ec264de22ce",
            "be7ca6a893d24bdca5488bd17fb4350e",
            "62943fee8add443798fa4a40f42afc3f"
          ]
        },
        "id": "7de42a2f",
        "outputId": "d2fed874-cbc4-409a-98f9-57ba32e4adcb"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d9ce88b3cbe46d0ad6c3447bebb3700",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/7708 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Embedding matrix shape: (7708, 200)\n"
          ]
        }
      ],
      "source": [
        "#!g1.1\n",
        "from glove import embedding_matrix_creator\n",
        "EMBEDDING_DIM = 200\n",
        "EMBEDDING = f\"GLV{EMBEDDING_DIM}\"\n",
        "\n",
        "embedding_matrix = embedding_matrix_creator(embedding_dim=EMBEDDING_DIM, word2idx=word2idx)\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0FPuwoZwD_TS",
      "metadata": {
        "id": "0FPuwoZwD_TS"
      },
      "outputs": [],
      "source": [
        "#!g1.1\n",
        "from metrics import *\n",
        "from utils_torch import words_from_tensors_fn\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "sentence_bleu_score_fn = bleu_score_fn(4, 'sentence')\n",
        "corpus_bleu_score_fn = bleu_score_fn(4, 'corpus')\n",
        "tensor_to_word_fn = words_from_tensors_fn(idx2word=idx2word)\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=train_set.pad_value)\n",
        "\n",
        "def evaluate_model(data_loader, model, data, bleu_score_fn, tensor_to_word_fn, desc=''):\n",
        "    running_bleu = [0.0] * 5\n",
        "    model.eval()\n",
        "    t = tqdm(iter(data_loader), desc=f'{desc}')\n",
        "    for batch_idx, batch in enumerate(t):\n",
        "        if data=='coco:':\n",
        "            images, captions = batch\n",
        "            images = images.to(device)\n",
        "        else:\n",
        "            images, captions, lengths = batch\n",
        "        outputs = tensor_to_word_fn(model.sample(images).cpu().numpy())\n",
        "\n",
        "        for i in (1, 2, 3, 4):\n",
        "            running_bleu[i] += bleu_score_fn(captions, outputs, n=i)\n",
        "        t.set_postfix({\n",
        "            'bleu1': running_bleu[1] / (batch_idx + 1),\n",
        "            'bleu4': running_bleu[4] / (batch_idx + 1),\n",
        "        }, refresh=True)\n",
        "    for i in (1, 2, 3, 4):\n",
        "        running_bleu[i] /= len(data_loader)\n",
        "    return running_bleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f552ef1a",
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install timm\n",
        "! gdown --id 123"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e6bf425",
      "metadata": {},
      "outputs": [],
      "source": [
        "NUM_HEADS_DEC = 10\n",
        "NUM_LAYERS_DEC = 6\n",
        "\n",
        "path = './vit_transformer_b50_emdGLV200_best_val_bleu.pt'\n",
        "checkpoint = torch.load(path)\n",
        "model = Captioner(NUM_HEADS_DEC, NUM_LAYERS_DEC, EMBEDDING_DIM, vocab_size, embedding_matrix, False).to(device)\n",
        "model.decoder.pos_encoder.pe = model.decoder.pos_encoder.pe.repeat(BATCH_SIZE, 1, 1).to(device)\n",
        "model.load_state_dict(checkpoint['state_dict'])\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ec42779",
      "metadata": {},
      "source": [
        "### Flickr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "805d9539",
      "metadata": {},
      "outputs": [],
      "source": [
        "#!g1.1\n",
        "\n",
        "inter_params = {\n",
        "    'model': model,\n",
        "    'bleu_score_fn': corpus_bleu_score_fn,\n",
        "    'tensor_to_word_fn': tensor_to_word_fn,\n",
        "    'data': 'flickr'\n",
        "}\n",
        "\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    train_bleu = evaluate_model(\n",
        "        desc=f'Train: ',\n",
        "        data_loader=train_eval_loader,\n",
        "        **inter_params,\n",
        "    )\n",
        "    \n",
        "    val_bleu = evaluate_model(\n",
        "        desc=f'Val: ',\n",
        "        data_loader=val_loader,\n",
        "        **inter_params,\n",
        "    )\n",
        "    \n",
        "    test_bleu = evaluate_model(\n",
        "        desc=f'Test: ',\n",
        "        data_loader=test_loader,\n",
        "        **inter_params,\n",
        "    )\n",
        "    for setname, result in zip(('train', 'val', 'test'), (train_bleu, val_bleu, test_bleu)):\n",
        "        print(setname, end=' ')\n",
        "        for ngram in (1, 2, 3, 4):\n",
        "            print(f'Bleu-{ngram}: {result[ngram]}', end=' ')\n",
        "        print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e721271",
      "metadata": {},
      "source": [
        "### COCO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49u4ZJhrEB6n",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49u4ZJhrEB6n",
        "outputId": "ea78e810-1684-4e7f-ed0b-8629c1de4f38"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tValidation Bleu Score: 100%|██████████| 406/406 [08:44<00:00,  1.29s/it, bleu1=0.000106, bleu4=0.0119]\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    val_bleu = evaluate_model(\n",
        "                    desc=f'\\tValidation Bleu Score: ',\n",
        "                    model=model,\n",
        "                    data='coco',\n",
        "                    bleu_score_fn=corpus_bleu_score_fn,\n",
        "                    tensor_to_word_fn=tensor_to_word_fn,\n",
        "                    data_loader=data_loader,\n",
        "                    vocab_size=vocab_size,\n",
        "                )\n",
        "\n",
        "    print('val', end=' ')\n",
        "    for ngram in (1, 2, 3, 4):\n",
        "        print(f'Bleu-{ngram}: {val_bleu[ngram]}', end=' ')\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "inference_transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "notebookId": "c2a3a5aa-b4c0-4f08-bbfc-6715c7555e93",
    "notebookPath": "image_captioning/Untitled.ipynb",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "236036b1f093498db63165faecd4df8d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "306fc3e5edad4009b772c9e35e84856d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66b5ec7cead44be4ba06a11f9808ce79",
            "max": 7708,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93dc6ed97b004f75a4d78ec264de22ce",
            "value": 7708
          }
        },
        "62943fee8add443798fa4a40f42afc3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "648aaeed9e2b4e38bd0bfe479345d106": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be7ca6a893d24bdca5488bd17fb4350e",
            "placeholder": "​",
            "style": "IPY_MODEL_62943fee8add443798fa4a40f42afc3f",
            "value": " 7708/7708 [00:00&lt;00:00, 57945.38it/s]"
          }
        },
        "66b5ec7cead44be4ba06a11f9808ce79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6d9ce88b3cbe46d0ad6c3447bebb3700": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a2d688389f4491db3a365ec0d37e806",
              "IPY_MODEL_306fc3e5edad4009b772c9e35e84856d",
              "IPY_MODEL_648aaeed9e2b4e38bd0bfe479345d106"
            ],
            "layout": "IPY_MODEL_236036b1f093498db63165faecd4df8d"
          }
        },
        "6df8886445454fc9ba211545a7002949": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93dc6ed97b004f75a4d78ec264de22ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a2d688389f4491db3a365ec0d37e806": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6df8886445454fc9ba211545a7002949",
            "placeholder": "​",
            "style": "IPY_MODEL_fcdfbefca4a0409692c75154bb4c4146",
            "value": "100%"
          }
        },
        "be7ca6a893d24bdca5488bd17fb4350e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcdfbefca4a0409692c75154bb4c4146": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
